# 1.2 实例

一个理解强化学习的好方法是了解一些实例或引导强化学习发展的可能的应用.

- 一个富有经验的象棋选手下了一步棋. 这个选择受到了两个方面的启发: 一是计划, 即预测可能的回击以及对回击的回击; 二是对特定位置与移动的价值的凭直觉的立即判断. 
- 一个自适应的控制器实时地调整炼油厂生产的各个参数. 控制器在设定的消耗的范围内, 优化产出/消耗/质量的之间权衡, 而不必使值严格地遵从工程师建议的预设值. 
- 羚羊幼崽在出生几分钟后就能挣扎着站起来. 半个小时后它就能以20英里每小时的速度奔跑.
- 一个能移动的机器人决定它应该是进入一个新房间来搜索更多的垃圾还是开始试图寻找回到充电站的路. 它基于当前的电量水平与过去它发现充电器的速度与难易度来做出决定.
- 菲尔准备他的早餐. 如果仔细检查的话, 即使是这样普通的活动也揭露出一张由条件行为和连锁的目标-子目标关系组成的复杂的网: 走向食物柜, 打开食物柜, 选择一个谷物盒, 然后将手伸向它, 抓住它, 最后将盒子取回. 其他复杂的、熟练的、交互的动作序列也被取回一个碗、汤匙或牛奶盒所需要. 每一步都涉及到一系列的眼球运动, 为了获取信息以及指导伸手与其他动作. 判断被迅速地做出: 怎样拿这个物体, 或在取别的物品前先将手头上的拿到餐桌上是否更好. 每一步都是由目标指导的, 比如抓一个汤匙或去冰箱那里, 并且每一步都是为其他的目标服务, 例如当谷物准备好后使用汤匙来吃它并最终获得营养. 无论他有没有意识到这一点, 菲尔一直在获取关于身体的状态信息, 这些状态信息确定了他的营养需求, 饥饿的等级, 以及事物上的偏好.

这些实例共有过于基本以致于容易被忽视的特点. 所有这些实例都涉及到做决策的代理及其环境间的*交互*, 在交互中虽然面对着环境中的*不确定性*但代理寻求达到特定的*目标*. 代理的动作能够影响环境的未来状态(例如下一步棋的位置, 炼油厂中储蓄池的油位, 机器人的下一个位置以及电池的未来电量), 因此能够影响代理后续可以采取的动作以及面对的机会. 正确的选择需要将间接的、延迟的动作序列考虑在内, 因此可能需要远见或计划.

与此同时, 在所有的这些实例中动作的效果不能被完全预测, 因此代理必须频繁地监视环境并作出合适的反应. 例如, 菲尔必须查看倒入碗中的牛奶的量以防止牛奶溢出. 所有这些实例涉及到一个明确的目标, 明确就明确在代理可以基于其能直接感知到的东西来判断离目标有多近. 棋手知道他是否赢了, 炼油厂控制器知道多少油正在被生产, 羚羊幼崽能察觉到它跌倒了, 机器人能判断它的电量是否耗尽了, 菲尔知道他是否享受他的早餐.

在所有的这些实例中, 代理能够逐渐地利用其经验来提升表现. 象棋选手提升了用于评估不同位置的直觉, 因此改善了他的下棋技巧; 羚羊幼崽提升了奔跑的效率; 菲尔学会了使用流水线的方式来制作早餐. 从一开始代理带入任务的知识——无论是从先前的关联任务获得的经验, 或通过设计或演化的方式来带入的——影响了哪些对于学习是有用的或者哪些容易学习, 但是同环境的交互对于调整动作来利用该任务的具体特征是必须的. 