# 1.3 强化学习的组成元素

越过代理与环境, 可以发现强化学习系统的四个主要模块: *策略*&lt;policy&gt;, *奖赏信号*&lt;reward signal&gt;, *值函数*&lt;value function&gt;, 以及可选的环境*模型*&lt;model&gt;.

*策略*定义了代理在给定时间的决策方式. 粗略地将, 策略就是从感知到的环境的状态, 到在这些状态下应该采取的动作的映射. 它对应于心理学中被称为刺激-反应规则/关联的集合. 在一些情况下可能是一个简单的函数或一张查找表, 但在其他的情况下可能涉及到例如搜索的额外操作. 从单策略就足以决定动作这一角度讲, 策略是强化学习代理的核心. 一般而言, 策略会以随机变量的形式展现, 即指定执行每一动作的可能性.

*奖赏信号*定义了强化学习问题的目标. 在每一时步&lt;time step&gt;中, 环境向强化学习代理发送一个称为*奖赏* &lt;reward&gt;的实数值. 代理的唯一目标就是最大化其长期的累积奖赏. 奖赏信号因此定义了对代理而言什么样的事件是好, 什么样的事件是坏. 对生物系统而言, 我们可以将奖赏类比于快乐或悲伤的体验. 奖赏信号是代理所面对的问题的直接的、决定性的特征.  奖赏信号是更改策略的基础; 如果被策略选择的动作只能带来低奖赏, 那么策略会被调整为: 将来在同样的情况下, 选择其他的动作. 一般而言, 奖赏信号是环境的状态与代理采取的动作的以随机变量为函数值的函数. 

然而奖赏信号只能显示立即的优劣, *值函数*&lt;value function&gt;才能指明长期的优劣. 粗略地讲, 一个状态的*值*&lt;value&gt;是从当前状态起, 代理未来所有奖赏的累积和的期望值. 奖赏只能决定对环境状态立即的、固有的喜好程度; 而值预示着从长期来看的对状态的喜好程度, 其中将未来可能遇到的状态以及可以从该状态中得到的奖赏也考虑在内. 例如, 一个状态可能只能获得较低的立即奖赏, 但如果其后能频繁遇到可以产生高奖赏的状态, 那么这个状态仍可能有较高的值. 如果拿人来做比方的话, 奖赏类似于快乐(如果较高)或痛苦(如果较低); 而值对应于当环境处于特定状态时, 对喜怒的更为精细与长远的考量.

从一定意义上说, 奖赏为主, 而作为对奖赏的预测的值为次. 没有奖赏就没有值, 对值作出评估的唯一目的就是获得更多的奖赏. 然而, 当评估并作出决定时, 值是我们最为关心的. 对动作的选择基于对值的评估. 我们寻求的动作应带来有最高值的状态, 而非带来最高奖赏, 因为从长远来看前者能带来最高的奖赏和. 然而不幸的是, 确定值远比确定奖赏困难的多. 从根本上说, 奖赏是由环境直接给出的; 而值必须在其整个生命周期中, 根据代理观察到的序列不停地评估与再评估. 事实上, 所有我们阐述的强化学习算法的最为重要的组件就是高效地对值进行评估的方法. 值估计的核心地位也许是过去六十年里关于强化学习的最为重要的结论. 

强化学习系统的第四也是最后一个元素是环境的*模型*&lt;model&gt;. 模型用于模仿环境的反应, 或更一般地讲, 其能够推断出环境将会做出怎样的反应. 例如, 给定一个状态和动作, 模型可以预测出作为结果的下一个状态以及奖赏. 模型用于*计划*&lt;planning&gt;, 计划的含义是在通过在实际经历前考虑将来可能的情形, 来决定行为方式. 使用模型与计划的强化学习方法被称为*有模型*&lt;model-based&gt;方法; 与之相反的是更简单的使用试错的*免模型*&lt;model-free&gt;方法, 试错可以视为计划的对立面. 在[第8章](to chapter8)中我们将介绍一强化学习系统, 其能同时做到通过试错学习, 学得环境模型, 以及将模型用于计划. 现代强行学习既包括低层的试错学习, 又包括高层的、周全的计划.