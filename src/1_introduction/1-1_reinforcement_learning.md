# 1.1 强化学习

强化学习是关于做什么——怎样将情形映射到动作——来最大化一个数字型的奖赏信号的学习. 学习器没有被告知什么动作应该被做, 而是通过尝试来发现什么样的动作可以产生最大的奖赏. 在最有趣也是最有挑战的情形下, 动作不仅会影响立即的奖赏, 而且会影响下一状态, 并通过这影响后续的奖赏. 这两个特性——试错性&lt;trial-and-error&gt;的搜索以及延迟的奖赏——是强化学习两个最为重要的且最具区分度的特征.

强化学习&lt;reinforcement learning&gt;, 像许多名字以"ing"结尾的主题——例如机器学习&lt;machine learning&gt;和登山&lt;mountaineering&gt;——一样, 即是问题, 也是一类在这个问题上效果很好的解决方法, 也是研究这一问题及其解决方法的领域的集合体. 使用一个名字来代表全部的三个事物是很方便的, 但同时也有必要将这三个从概念上分开. 特别需要注意的是, 在强化学习中问题与解决方法的区别是很重要的; 不能做出这个区分是很多困惑的源头.

我们使用动力系统理论&lt;dynamical systems theory&gt;中的概念来形式化强化学习问题, 更具体地说, 将强化学习问题作为非完全已知的马尔科夫决策过程&lt;Markov decision process, MDP&gt;的最优化控制. 此形式化的细节必须等到[第3章](to chapter3)呈现, 但是基本的想法仅仅是捕捉"学习代理&lt;learning agent&gt;怎样持续地和环境交互来达到目标"这一问题的最重要的方面. 学习代理必须能够从某种程度上察觉到环境的状态, 也必须能够做出行动来影响环境的状态. 代理也必须拥有一个或多个和环境的状态相关的目标. 马尔科夫决策过程仅仅以最简单的方式包含了这三个方面——感知, 动作与目标——且没有贬低任一方面的重要性. 任何适宜于解决这样的问题的方法可以被认作是强化学习方法.

强化学习不同于*监督学习*&lt;supervised learning&gt;, 后者在机器学习领域的大多数当前研究中探讨. 监督学习是从由外在的有领域知识的监督者&lt;supervisor&gt;提供的标记好的实例组成的训练集中获得知识. 每一个实例是对一个情形的描述以及在这种情形下的系统应该做的规范(或标签), 系统做的常常为将当前的情形归到哪个类别下. 这类学习的目标是对系统的响应进行推算或泛化, 使其能在没有于训练集中出现过的情形下做出正确的响应. 这是一类重要的学习, 但仅有它的话对从交互中学习而言是不够的. 在交互式的问题中获得既正确又能代表代理不得不做出反应的所有情形的行为的实例是不现实的. 在未知的、想要让学习最为有效的领域中, 一个代理必须从它自己的经历中学习.

强化学习又不同与机器学习研究者所称的*无监督学习*&lt;unsupervised learning&gt;, 后者常常是用于发现未标签的数据集合的潜在结构. 监督学习与无监督学习这两个术语看上去能够将机器学习的范式进行彻底地分类, 但事实上并非如此. 虽然有些人可能忍不住将强化学习视为无监督学习的一种, 但强化学习试图最大化奖赏信号而非发现隐藏的结构. 对于强化学习来说发现代理经历中的结构当然是有用的, 但其本身没有解决最大化奖赏信号这一强化学习问题. 因此除监督学习与无监督学习以及可能的其他范式之外, 我们认为强化学习是第三种机器学习范式. 

在其他类型的学习问题中没有出现而出现在强化学习中的挑战之一, 是探索&lt;exploration&gt;与利用&lt;exploitation&gt;的权衡. 想要获得许多奖赏, 一个强化学习代理必须偏爱已经尝试过并且被发现可以产生高奖赏的动作. 但为了发现这样的动作, 代理不得不之前未选择过的动作. 一方面, 代理不得不*利用*&lt;exploit&gt;已有的经验来获得奖赏; 另一方面, 代理不得不*探索*&lt;explore&gt;以便能在将来做出更好的动作选择. 两难的境地就是只追求探索或只追求利用都不能完全任务. 代理必须尝试各种动作, 然后逐渐偏向于看上去最好的那个. 在一个涉及到随机变量的任务中, 一个动作必须被尝试许多次以获得其期望奖赏的可靠估计. 探索-利用困境已经被数学家深入地研究了几十年, 但依然尚未解决. 目前, 我们只需要知道平衡探索与利用的整个问题甚至没有在监督学习和无监督学习中出现, 至少在这些范式最典型的情形下.

强化学习的另一个关键特征是其明确地目标导向的、代理同不确定的环境交互的问题作为一个整体考虑. 这与许多只考虑子问题而不考虑怎么令其适应于更大的版图. 例如, 我们提到过许多机器学习研究关注于没有明确指明最终作用的监督学习问题. 其他研究者发展出有通用目标的计划&lt;planning&gt;理论, 但没有考虑计划在实时决策中的角色, 也不关心对计划而言必要的预测模型从哪儿来这一问题. 虽然这些方法已经产生了许多实用的结果, 他们对割裂开来的子问题的关注是一个巨大的限制.

强化学习采取了相反的方针, 即从完整的、交互式的、追寻目标的代理开始. 所有的强化学习代理都拥有明确的目标, 能够感知到环境中的各个方面, 也能够选择做出各种动作来影响环境. 此外, 常常从开始就假设, 即使面临着关于环境的巨大不确定性, 代理也不得不做出反应. 当强化学习涉及到计划时, 必须处理计划与实时动作选择之间的相互影响以及环境模型怎么获得与改进这一问题. 当强化学习牵涉到监督学习时, 常常用监督学习确定哪些能力是至关重要的以及哪些能力是不重要的. 如果想要强化学习的研究有进展, 重要的子问题应该被独立开来研究, 但是其应该在完整的、交互式的、追寻目标的代理中扮演清晰的角色, 即使代理的全部细节还没有被补充完整. 

完整的、交互式的、追寻目标的代理并不总是意味着类似完整的生命体或机器人之类的物体. 这些是很清晰的例子, 但完整的、交互式的、追寻目标的代理也可以是一个更大的行为系统的组件. 在这种情况下, 代理直接与这个更大的系统的其余部分交互, 并间接地和更大的系统的环境交互. 一个简单的例子是这样一个代理, 其监视机器人的电量水平并向机器人的控制组件发送命令. 代理的环境是机器人的余下部分以及机器人所处的环境. 只有目光超越代理与其环境的最明显的示例, 才能意识到强化学习框架的通用性. 

与其他的科技领域的大量而富有成果的交互是现代强化学习最令人激动的一面. 在人工智能与机器学习的领域中, 强化学习也是持续了数十年的, 寻求与统计学、优化理论以及其他数学科目的更好集成的趋势的一部分. 例如, 一些使用参数化近似器&lt;parameterized approximator&gt;的强化学习方法的能力可以应对在运筹学与控制论中的典型的"维数灾难"&lt;curse of dimensionality&gt;问题. 更特别的是, 强化学习一直与心理学与神经科学有紧密的交互, 且各交互的双方都有巨大的收获. 在各种形式的机器学习中, 强化学习是与人类和其他动物的学习方式最接近的, 并且许多强化学习的核心算法最初是受到了生物学习系统的启发. 而强化学习既通过与实验数据符合得更好的关于动物学习的心理学模型予以回报, 也通过关于大脑的部分奖励系统的富有影响力的模型予以回报. 本书的主体阐述属于工程学与人工智能的强化学习的理念, 而强化学习和心理学以及神经科学的联系将在[第14章](to chapter14)和[第15章](to chapter15)进行概述.

最后, 强化学习也是在人工智能方面回归简单通用准则的的更大的潮流的一部分. 自从20世纪60年代晚期, 许多人工智能研究者假设没有通用的准则能被发掘, 假设相反智能是由于大量特殊目的的技巧、过程与启发式算法的存在. 有时有这样的说法: 如果能将足够多的相关的事实, 例如数百万条或数十亿条, 放进一台机器里, 那么这台机器就能拥有智能. 基于通用准则的方法, 例如搜索与学习, 被称为"弱方法"&lt;weak method&gt;, 反之基于特定知识的被称为"强方法"&lt;strong method&gt;. 这个观点在今日依然很常见, 但并非统治性的. 就我们的观点而言, 这样的假设言之过早: 投入到通用准则的研究中的努力太少了, 因此不能得出通用准则不存在这样的结论. 现代人工智能目前包括大量的关于学习、搜索以及决策的通用准则的研究. 目前仍不确定历史的钟摆会往回摆多少, 但强化学习毫无疑问是更简单与更少的人工智能通用准则这一回摆的一部分.