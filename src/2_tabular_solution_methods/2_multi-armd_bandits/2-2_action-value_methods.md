# 2.2 动作值方法

<link href="../../css/style.css" rel="stylesheet"></link>
我们以对两种方法的更进一步的审视开始, 这两种方法分别为估计动作值的方法以及使用估计值来做出动作选择的决策的方法, 两者合称为*动作值方法*&lt;action-value method&gt;. 我们还记得一个动作的真实值是该动作被选择时的平均奖赏. 一个自然的估计方法就是对接受到的奖赏进行平均:

$$Q_t(a) = \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} = \frac{\sum_{i=1}^{t-1}{R_i} \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1}{\mathbb{1}_{A_i = a}}}, \tag{2.1}$$

其中$$\mathbb{1}_{predicate}$$表示一随机变量, 当谓词&lt;predicate&gt;为真时其值为1, 反之为0. 如果分母为0的话, 那么我们可以将$$Q_t(a)$$设定为默认值, 如0. 当分母趋向于无穷大时, 由大数定理可以得知$$Q_t(a)$$收敛于$$q_*(a)$$. 我们将此称为估计动作值的*样本平均*方法&lt;sample-average method&gt;, 因为估计值为相关奖赏的样本的均值. 当然这只是估计动作值的一种方法, 也不一定是最好的一种. 但是, 让我们暂且使用这种简单的估计方法, 然后考虑怎样使用估计值来选择动作的问题.

最简单的动作选择规则就是选择有最高估计值的那个动作, 即前一节中定义的贪心动作. 如果有多于一个的贪心动作, 那么以任意一种方式在选择其中一个, 例如随机选择. 我们将这样的*贪心*&lt;greedy&gt;动作选择写作

$$A_t = \underset{a}{\operatorname{argmax}} Q_t(a), \tag{2.2}$$

其中$$\operatorname{argmax}_a$$表示令后面跟随的表达式最大的那个动作$$a$$(再次声明, 如果有多个最值时任意选择). 贪心选择总是对已有的知识进行利用来最大化立即的奖赏; 其不会对明显次等的动作进行采样来观察这些动作是否实际上更好. 一个简单的替代方法就是在多数的时间内进行贪心选择; 但是每隔一定时间, 如以一个较小的概率$$\varepsilon$$, 从所有的动作中以相同的概率进行随机选择, 无论各个动作的估计值为多少. 我们将使用这种近似贪心的动作选择规则的方法称为$$\varepsilon$$-贪心方法. 这一方法的优点是, 当步数增加到无穷大时, 每个动作都会被采样无穷多次, 因此保证了所有的$$Q_t(a)$$收敛到$$q_*(a)$$. 这也预示着选择最优动作的概率会收敛到大于$$1 - \varepsilon$$的值, 即几乎确定. 然而这只是一个渐进的保证, 且无法说明该方法的实际效率. 

*练习2.1*&nbsp; &nbsp; &nbsp; &nbsp; 对于$$\varepsilon$$-贪心动作选择, 在有两个动作并且$$\varepsilon = 0.5$$的情况下, 贪心动作被选择的概率是多少.<span class="float_right">$$\square$$</span>