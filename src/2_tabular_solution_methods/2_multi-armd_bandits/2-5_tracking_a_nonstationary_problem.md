# 2.5 追踪非固定性问题

我们至今讨论的平均方法可以适用于固定性赌博机问题, 即奖赏的概率分布不会随时间变化的赌博机问题. 就像之前指出的那样, 实际上我们经常遇到非固定性的强化学习问题. 在这种情况下, 相对于较早的奖赏, 将更多的权重给予较近的奖赏是很有意义的. 最为流行的、能做到这一点的方法之一为使用固定的步长参数. 例如, 对过去的$$n - 1$$次奖赏的平均$$Q_n$$, 其增量式更新规则[(2.3)](to equation2.3)可以改写为
$$
Q_{n + 1} \doteq Q_n + \alpha [R_n - Q_n],
\tag{2.5}
$$
其中步长参数$$\alpha \in (0, 1]$$, 为一常数. 这使得了$$Q{n + 1}$$成为了对过去奖赏与初始估计值$$Q_1$$的加权平均:
$$
\begin{align}
Q_{n + 1} &= Q_n + \alpha [R_n - Q_n] \\
&= \alpha R_n + (1 - \alpha) Q_n \\
&=  \alpha R_n + (1 - \alpha) [\alpha R_{n - 1} + (1 - \alpha)Q_{n - 1}] \\
&=  \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 Q_{n - 1} \\
&=  \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} + \dots + (1 - \alpha)^{n - 1} \alpha R_1 + (1 - \alpha)^n Q_1 \\
&= (1 - \alpha)^n Q_1 + \sum_{i = 1}^n \alpha (1 - \alpha)^{n - i} R_i .
\tag{2.6}
\end{align}
$$
我们将此称为加权平均, 因为权重之和$$(1 - \alpha)^n + \sum_{i = 1}^n \alpha (1 - \alpha)^{n - i} = 1$$, 读者可以自行证明. 请注意给予奖赏$$R_i$$的权重$$\alpha (1 - \alpha)^{n - i}$$取决于多少次奖赏前该奖赏被观察到, 即$$n - 1$$的值. $$1 - \alpha$$的值小于1, 因此给予$$R_i$$的权重会随着与迄今间隔的奖赏次数增加而减少. 事实上, 因为以$$(1 - \alpha)$$为底的指数因子的存在, 该权重会指数衰减(如果$$1 - \alpha = 0$$, 那么所有的权重都集中于最新的奖赏$$R_n$$, 因为依照惯例$$0^0 = 1$$). 所以, 这有时也被称为*指数新近加权平均*&lt;exponential recency-average&gt;.

有时, 在每一步都改变步长参数可以提供便利. 令$$\alpha_n (a)$$表示用于处理第$$n$$次选择动作$$a$$后收到的奖赏的步长参数. 如我们之前所说, 若$$\alpha_n (a) = \frac{1}{n}$$即为采样平均方法, 而依据大数定理其确保能收敛到真实的动作值. 但理所当然, 不是所有的序列选择$${\alpha_n(a)}$$都确保能收敛. 一个随机逼近理论&lt;stochastic approximation theory&gt;中的著名结论, 为我们提供了能以1的概率保证收敛的条件:
$$
\sum_{n = 1}^\infty \alpha_n(a) = \infty  \; \text{and} \; \sum_{n = 1}^\infty \alpha_n^2(a) < \infty
\tag{2.7}
$$
我们需要第一个条件来保证这些步长对最终克服初始条件或随机波动的影响而言足够大. 第二个条件确保最终步长变得足够小以确保收敛. 

请注意采样平均的情形, 即$$\alpha_n(a) = \frac{1}{n}$$, 满足上述两个条件; 而步长恒定的情形, 即$$\alpha_n(a) = \alpha$$, 并非如此. 对后者而言, 其并不能满足第二个条件, 意味着估计值不会完全收敛, 而是继续随新收到的奖赏变化. 就像我们之前提到过的那样, 这事实上正是非固定性环境所需要的, 而实际上非固定性问题在强化学习中是最为常见的. 此外, 满足条件[(2.7)](to equation2.7)步长参数序列常常收敛得非常缓慢或需要大量的调参来获得令人满意的收敛速率. 虽然满足上述收敛条件的步长参数序列常常用于理论研究中, 其极少用于实际应用或实证研究中.

*练习 2.4*&nbsp; &nbsp; &nbsp; &nbsp;如果步长参数$$\alpha_n$$不是常数, 那么估计值$$Q_n$$为对之前收到的奖赏的加权平均, 且权重与[(2.6)](to equation2.6)给出的不同. 那么类比于[(2.6)](to equation2.6), 怎么使用步长参数序列, 来表示一般情形下的各个已收到的奖赏的权重?<span class="float_right">$$\square$$</span>

*练习 2.5 (编程)*&nbsp; &nbsp; &nbsp; &nbsp;设计并实现一个实验来, 来解释样本平均方法在处理非固定性问题时面临的困境. 可以使用10-摇臂测试工具的修改版本, 其中所有的$$q_*(a)$$初始值相同, 然后独立地随机变动(例如在每一步, 向各个$$q_*(a)$$添加采样自均值为0且标准差为0.01的正态分布的增量). 然后为使用样本平均法的、增量式计算的动作值方法, 和使用恒定步长参数, 即$$\alpha = 0.1$$的动作值方法, 各自绘制如[图2.2](to figure2.2)的图表. 令$$\varepsilon = 0.1$$, 并且使用更长的行程, 例如10,000步.<span class="float_right">$$\square$$</span>