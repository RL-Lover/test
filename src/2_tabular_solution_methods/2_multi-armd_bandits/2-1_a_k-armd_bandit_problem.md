# 2.1 k-摇臂赌博机问题

考虑如下的学习问题. 你需要重复地对$$k$$个不同的选项或动作做出选择. 在每一次选择后你会获得一个数值型的奖赏, 该奖赏是从由你选择的动作所确定的、固定的概率分布中采样获得的. 你的目标在一定的时期内, 如1000个动作选择或*时步*&lt;time step&gt;内, 最大化期望的奖赏和.

这是$$k$$*-摇臂赌博机问题*&lt;$$k$$-armed bandit problem&gt;的典型形式, 之所以这么称呼是类比于老虎机或"单摇臂赌博机", 只不过其有$$k$$个摇臂, 而非1个. 每一次动作选择就像拉下赌博机的摇臂之一, 而奖赏奖赏就是中了头奖之后的回报. 在反复的动作选择过程中, 你必须将动作集中到最好的摇臂上来最大化累积奖赏. 另一个类比为医生为一批批的重病患者选择实验性的疗法. 每一个动作就是选择一种疗法, 而每一个奖赏就是病人存活或健康与否. 现如今术语"赌博机问题"有时也用于上述问题的泛化, 但本书中我们仅用其指代上述的简单形式.

在我们的$$k$$-摇臂赌博机问题中, $$k$$个动作中的每一个动作都各自有其期望或平均的奖赏; 让我们称其为该动作的*值*&lt;value&gt;. 我们将在时步$$t$$选择的动作记为$$A_t$$, 对应的奖赏记为$$R_t$$. 任一动作$$a$$的值, 记为$$q_*(a)$$, 为$$a$$被选择后的期望奖赏:

$$q_*(a) \doteq \mathbb E [R_t \mid A_t = a]$$.

如果你知道了每个动作的值, 那么解决$$k$$-摇臂赌博机问题就很简单了: 只要一直选择值最高的动作即可. 我们假设你不能确定动作值, 虽然你可能有其估计值. 我们将动作$$a$$在时步$$t$$的估计值记为$$Q_t(a)$$. 我们希望$$Q_t(a)$$尽可能接近$$q_*(a)$$.

如果你维持有对动作值的估计, 那么在任何时步一定至少有一个动作有着最高的估计值. 我们将其称为*贪心*&lt;greedy&gt;动作. 当你选择贪心动作之一时, 我们称你在*利用*&lt;exploit&gt;你对动作值的已有知识. 但如果你选择了非贪心动作之一, 那么我们称你在*探索*&lt;explore&gt;, 因为这能帮助你改进对非贪心动作的值的估计. 利用用于最大化单步的期望奖赏,  但探索也许可以在长期内产生更高的奖赏和. 例如, 假设一个贪心动作的值是确定地知道的, 而一些其他的动作以很高的不确定性被估计和贪心动作差不多好. 这个不确定性中包含了至少这些动作中的一个, 可能比贪心动作要好, 但你不知道是哪一个. 如果你在选择动作前有许多时步的话, 那么也许这么做更好: 探索非贪心动作并发现其中哪个比贪心动作更好. 在探索过程中, 短期而言奖赏变低了, 但长期而言奖赏会变高, 因为在你发现了更好的动作后, 你可以穿多次地利用*它们*. 因为在单个动作选择中不能既探索又利用, 所有这常常被称为探索与利用之间的"矛盾".

在任何一个具体的情况下, 是探索还是利用更好取决于对估计值、不确定性、余下步数的具体值的复杂考量. 有许多针对$$k$$-摇臂赌博机及关联问题的特定数学形式的, 用于平衡探索和利用的精巧方法. 然而, 这些方法中的多数对固定性&lt;stationarity, 动作的真实值不会随时间变化&gt;以及先验知识做出了较强的假设, 这些假设对应用或我们在接下来的章节中考虑的完整的强化学习问题而言, 要么无法做到, 要么无法证实. 当这些假设不成立时, 就这些方法而言, 最优解的保证或损失的边界都无从谈起. 

在本书中我们不考虑以精妙的方式平衡探索和利用; 我们仅对浅显的平衡进行探讨. 在本章中我们将呈现数种针对$$k$$-摇臂赌博机问题的平衡方法, 以及其对仅利用的方法的显著优越性. 需要平衡探索和利用是强化学习特有的特有的挑战; 我们这一版本的$$k$$-摇臂赌博机问题的简洁性, 使我们能够以一种特别清晰的形式来呈现这一点.