# 第一部分 表格式解决方法

在本书的这一部分, 我们在强化学习最简单的形式下——状态与动作空间足够小使得值函数估计值可以以数组或*表*&lt;table&gt;来表示——阐述几乎所有强化学习算法的核心概念. 在这种情况下, 这些方法常常可以获得确切的解, 即可以获得确切的最优值函数与最优策略. 这和本书的下一部分叙述的近似方法恰恰相反, 后者只能获得近似解, 但作为回报可以高效地应用于规模大得多的问题上.

本书这一部分的第一章阐述了只有单个状态的强化学习问题特例, 即所谓的赌博机问题. 第二章阐述了在本书的余下部分使用的、一般强化学习问题的形式化——有限马尔科夫决策过程&lt;finite Markov decision process, finite MDP&gt;——及其包括了贝尔曼等式&lt;Bellman equation&gt;与值函数的主要概念.

接下来的三章阐述了三类解决有限马尔科夫决策过程的基本方法: 动态规划&lt;dynamic programming, DP&gt;, 蒙特卡洛&lt;Monte Carlo, MC&gt;方法, 以及时序差分&lt;temporal difference, TD&gt;方法. 每一类方法都有其优点与缺点. 动态规划方法在数学上被研究得很好, 但需要完整、正确的环境模型. 蒙特卡洛方法不需要模型且从概念上说较为简单, 但不适合于逐步的增量计算. 最后, 时序差分方法不需要模型且完全是增量式的, 但分析起来更为复杂. 这些方法在效率与收敛速度这些方面也存在着差异.

余下的两章阐述了怎么将这三类方法结合起来来利用各自的优点. 在其中一张我们阐述怎样通过多步自举方法&lt;multi-step bootstrapping method&gt;来将蒙特卡洛方法与时序差分方法的长处结合起来. 在本部分的最后一章, 我们将展示怎样将时序差分学习方法, 与模型学习以及计划方法(例如动态规划)结合起来, 作为完整、统一的表格式强化学习问题的解决方案.