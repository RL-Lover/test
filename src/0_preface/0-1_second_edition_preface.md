# 第二版序

由本书的第一版出版至今的二十年见证了人工智能领域的巨大进步, 该进步很大程度上由机器学习的发展推动, 而机器学习的发展也包括了强化学习的发展. 虽然计算能力的增强为这些发展做出了贡献, 但理论与算法上的新进展也同样是强大的驱动力. 面对这样的进步, 相较于1998年版的第二版是期待已久的, 于是我们终于在2012年开始了这个项目. 本书第二版的目标同第一版是一致的: 提供清晰明了的、可被关联领域的读者理解的强化学习关键思想与算法的阐述. 这个版本依然是导论性质的, 并且我们仍将注意力集中于核心的、在线&lt;online&gt;学习算法. 这个版本包含了一些在间隔的这些年中重要性凸显的主题, 并且我们拓展了对我们现在理解得更好的主题的涵盖. 但是我们不会尝试提供对强化学习领域的全面的覆盖, 该领域在许多不同的方向上有爆炸式的发展. 我们为不得不遗漏其中的大部分而只能保留小部分而道歉.

就像在第一版中做的那样, 我们选择不以严密、正式的方式阐述强化学习, 不对其在最广泛的情形下进行形式化. 然而, 自第一版以来我们在一些主题上形成的更深入的理解需要更多的些许数学来解释; 我们将需要更多数学知识的部分用带阴影的方框隔开, 以便苦于数学的读者可以选择跳过. 我们也使用了和第一版略微不同的数学标记. 在教学的过程中, 我们发现新的标记可以解决一些普遍的疑惑点. 这套标记系统强调了随机变量与其实例的区别, 其中前者使用大写字母标记, 后者使用小写字母. 举个例子, 在第t步的状态&lt;state&gt;, 动作&lt;action&gt;与奖赏&lt;reward&gt;分别被标记为$$S_t$$, $$A_t$$与$$R_t$$,而它们可能的取值可以被标记为$$s$$, $$a$$, $$r$$. 此外, 很自然的将小写字母用于值函数&lt;value function&gt;(比如$$v_{\pi}$$), 并将大写字母限定于标记它们表格式(tabular)的估计值.  近似值函数&lt;approximate value function&gt;是任意参数的确定性&lt;deterministic&gt;函数, 因此也使用小写字母标识(例如$$\hat v (\mathbf s, \mathbf w_t) \approx v_\pi(\mathbf s)$$). 向量,  例如权重向量$$\mathbf w_t$$(或更为正式的**$${\boldsymbol \theta}_t$$**)以及特征向量$$\mathbf x_t$$(或更为正式的$$\boldsymbol \phi_t$$)都是粗体并用小写字母标识, 即使它们是随机变量. 大写的粗体为矩阵保留. 在第一版中我们使用专用的标记$$ \mathfrak P_{s, s^\prime}^a$$和$$\mathfrak R_{s, s^\prime}^a$$ 分别标识转移概率与奖赏的期望值. 上述标识的一个缺点是其仍然不能完全描述奖励的动态&lt;dynamics, 和MDP状态转移相关的性质&gt;, 因为其只给出了奖励的期望值: 这对动态规划&lt;dynamic programming&gt;而言是足够的, 但对强化学习不足. 上述标识的另一个缺点是对下标与上标的滥用. 在本版中我们使用$$p(s^\prime, r \mid s, a)$$这一明确的标识来表示给定当前状态与动作的情况下, 下一状态与奖赏的联合概率. 所有标识的变化都列于[标识一览](to notification)中.

本版极大地进行了拓展, 并且顶层的组织发生了改变. 在介绍性质的第一章后, 本版被划分为三个新部分. 第一部分(第2章到第8章)尽可能多地介绍表格式情形下的强化学习, 在这种情况下确定的解是可以获得的. 我们涵盖了在表格式情形下的学习&lt;learning&gt;与计划&lt;planning&gt;方法, 以及两者在n-步&lt;n-step&gt;方法及Dyna中的统一. 许多呈现在这一部分的算法是第二版新增的, 包括UCB, 期望Sarsa&lt;Expected Sarsa&gt;, 双重学习&lt;Double Learning&gt;, 树回溯&lt;tree-backup&gt;, $$Q(\sigma)$$, RTDP以及MTCS. 先在表格式的情况下进行彻底的展开, 使得核心概念能在最简单的设定下进行阐述. 本书的第二部分(第9章到第13章)致力于将这些概念拓展到函数近似&lt;function approximation&gt;的情况下. 其新增了关于以下内容的新的章节: 人工神经网络, 傅里叶基函数, LSTD, 核方法, 梯度-TD&lt;Gradient-TD&gt;与强调-TD&lt;Emphatic-TD&gt;方法, 平均奖励&lt;average-reward&gt;方法, 真在线TD($$\lambda$$) &lt;true online TD($$\lambda$$)&gt;, 以及策略梯度&lt;policy-gradient&gt;方法. 本版极大地拓展了异策略学习&lt;off-policy learning&gt;的内容, 首先是在第5到7章的表格式的情形下, 其次是在第11, 12章的函数近似情形下. 本版中的另一个变化是将n-步自举&lt;n-step bootstrapping&gt;的前向视角&lt;forward-view&gt;方法(现在在[第7章](to chapter7)中被更详细地阐述), 与使用资格迹&lt;eligibility trace&gt;的后向视图&lt;backward-view&gt;(现在在[第12章](to chapter12)中独立阐述)分离开来. 本书的第三部分有关于以下内容的大量新章节: 强化学习与心理学([第14章](to chapter14))和神经科学([第15章](to chapter15))的关联, 以及包括了雅达利&lt;Atari&gt;游戏、Watson的赌博策略以及围棋程序AlphaGo与AlphaGo Zero的案例学习章节([第16章](to chapter16)). 像上版那样, 由于必要性使然, 我们只覆盖了强化学习领域的所有成就的一小部分. 我们的选择折射出我们对于计算上廉价的无模型&lt;model-free&gt;方法的长期兴趣, 其能够很好地拓展到大型应用上. 最后一章现在涵盖了关于强化学习未来的社会影响的讨论. 

本书被设计为一到两学期的强化学习课程的基本参考书. 对于一学期的课程, 前十章应该被涵盖以便形成一个良好的课程核心, 在这之上可以根据教学口味添加其他章节的材料, 可以添加来自其他参考书, 如Bertsekas and Tsitsiklis(1996), Wiering and van Otterlo(2012)与Szepesári(2010)的材料, 或者可以添加来自文献的材料. 视学生的背景而定, 一些额外的在线监督学习的材料可能是有用的. 选择&lt;option&gt;与选择模型&lt;option model&gt;的理念也是一个自然的附加项(Sutton, Precup and Singh, 1999). 两学期的课程可以涵盖所有的章节以及补充材料. 本书也可作为更宽泛的课程——如机器学习、人工智能及人工神经网络——的一部分. 在这种情况下, 可能只需要涵盖本书的部分内容. 我们推荐涵盖作为简短概述的第1章, 第2章中直到2.4节(包括)的内容, 以及第3章, 然后根据时间与兴趣从余下部分选择章节. 第6章是对整个课题与余下内容而言最为重要的部分. 专注于机器学习或人工神经网络的课程应该涵盖第9章与第10章, 而专注于人工智能或规划的课程应该涵盖第8章. 在整本书中, 更为困难且对余下内容并非必要的章节被标上了\*. 这些内容可以在首次阅读的时候被忽略, 且不会对阅读后续的内容产生影响. 一些练习题也被标上了\*以显示其更为深入, 并且对理解那一章的基本内容不是必需的. 

大多数的章以题为"参考文献与历史沿革"的节结束, 在其中我们阐明该章中出现的概念的源头, 提供对更深入的阅读与正在进行的研究的指引, 并且叙述相关的历史背景. 虽然我们想使这些节权威与完整, 但我们毫无疑问地遗漏了一些重要的先驱性工作. 对此我们再次致以歉意, 同时我们欢迎指正与拓展, 作为对本书的电子版的合作的一部分.

像第一版一样, 仅以本书的第二版缅怀A. Harry Klopf. 正是Harry将我们介绍给彼此, 同时正是他关于大脑与人工智能的想法开启了我们走近强化学习的漫长旅途. 接受过神经生理学的学习并且长期地对机器智能拥有兴趣, Harry是一位隶属于位于俄亥俄州Wright-Patterson空军基地的空军航电指挥部研究所&lt;Avionics Directorate of the Air Force Office of Scientific Research, AFOSR&gt;的资深科学家. 他不满于在解释自然智能与为机器智能提供基石方面, 巨大的重要性被赋给了平衡发现&lt;equilibrium-seeking&gt;过程, 其中包括内平衡与误差纠正的模式分类法. 他指出试图最大化某个量(无论这个量是什么)的系统与平衡发现系统有本质上的区别, 并且他主张最大化系统是理解自然智能的某些重要方面以及人工智能系统的关键所在. Harry促成了从AFOSR处获得资金, 来开展评估上述以及相关观点的科学价值的项目. 这个项目在于70年代晚期在UMass Amherst展开, 该项目最初由Michael Arbib, William Kilmer以及Nico Amherst指导, 他们既是UMass Amherst的计算机与信息科学系的教授, 同时也是系统神经科学属神经机械学中心的创始人, 这是一个专注于神经科学与人工智能的交叉领域的有远见的团队. Barto, 作为刚毕业于密歇根大学的Ph.D, 被聘请为项目的博士后研究员. 与此同时, Sutton, 作为一个学习计算机科学与心理学的斯坦福大学本科生, 一直与Harry就刺激的时序&lt;timing&gt;在经典条件作用&lt;classical conditioning&gt;中扮演的角色这一共同兴趣进行通信. Harry向UMass的团队建议Sutton也应该加入到项目中. 因此Sutton成为了UMass的研究生, 他的博士导师是已经成为副教授的Sutton. 呈现在本书中的关于强化学习的研究, 正可以说是这个由Harry发起并受到他的思想启发的项目的产物. 更进一步讲, 正是Harry将我们, 两位作者, 带入到了一段长而令人享受的友谊中. 仅以此书献给Harry以纪念他必不可少的贡献, 这贡献不仅是对强化学习领域的, 也是对我们友谊的. 我们也要感谢Arbib教授, Kilmer教授以及Spinelli教授为我们提供了探索这些想法的机会. 最后, 我们要感谢AFOSR在对我们早年的研究的慷慨援助, 以及NSF在此后的许多年中的慷慨援助.

我们必须感谢许多人, 感谢他们在这第二版上提供的启示与帮助. 为第一版提供启示与帮助的、我们曾致谢的每一个人, 同样也值得我们在第二版上为他们致以最深沉的感谢, 因为如果没有他们对第一版所做的贡献的话, 本版也就不会存在了. 我们也必须将为第二版做出了特别的贡献的许多其他人加入到致谢列表中. 过去这些年我们使用这份材料教的学生, 在无数方面做出了贡献: 暴露出书中的错误, 提供解决方案, 以及同样重要的, 提出对我们本可以解释得更好的地方的困惑. 我们特别感谢Martha Steenstrup, 因为他从头至尾地阅读本书并提供了详尽的评论.  如果不是许多在这些领域的专家的帮助的话, 关于心理学与神经科学的章节是不可能完成的. 我们向John Moore致谢, 为他在许许多多年中关于动物学习的实验、理论以及神经科学耐心指导, 也为他对第14与15章的许多草稿的仔细阅读. 我们同样也要Matt Botvinick, Nathaniel Daw, Peter Dayan和Yael Niv, 为他们在这些章节的草稿上提供的富有洞察力的评论、他们在大量的文献上提供的必不可少的指导以及他们对我们早期的草稿中许多错误的拦截. 当然, 在这些章节中遗留的错误——一定还遗留了一些——完全都是我们的锅. 我们感谢Phil Thomas, 因为他帮助我们使这些章节能被非心理学与神经科学专业的读者理解; 我们也感谢Peter Sterling, 因为他帮助我们改善了论述. 我们对Jim Houk充满了感激之情, 因为他向我们介绍了基底核&lt;basal ganglia&gt;中信息处理的课题, 也向我们提醒了神经科学的相关方面. José Martínez, Terry Sejnowski, David Silver, Gerry Tesauro, Georgios Theocharous以及Phil Thomas帮助我们理解他们的强化学习应用中的细节以便能将这些内容包含在"案例学习"这一章中, 他们也为这些章节的草稿提供了建设性的意见. 特别的感谢被致以David Silver, 因为他更好地帮助我们理解了MCTS与DeepMind的围棋程序. 我们感谢George Konidaris在傅里叶基函数这一节中提供的帮助. 我们感谢Emilio Cartoni, Thomas Cederborg, Stefan Dernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon, 因为他们在一些最重要的、我们最为感谢的方面的提供的帮助.

Sutton也想要感谢University of Alberta的强化学习与人工智能实验室的同事以及他们为本版做出的贡献. Sutton必须特别感谢Rupam Mahmood, 为他在[第5章](to chapter5)的异策略&lt;off-policy&gt;蒙特卡洛方法方面做出的贡献; 感谢Hamid Maei, 为他对[第11章](to chapter11)呈现的异策略学习的更进一步的观点提供的帮助;  感谢Eric Graves, 为他所做的[第13章](to chapter13)中的实验; 感谢Shangtong Zhang, 为他对几乎所有的实验的重做与实验结果的验证; 感谢Kris De Asis, 为他[第7章](to chapter7)与[第12章](to chapter12)的关于新技术的内容的内容的改进; 感谢Harm van Seijen, 为他将n-步方法与资格迹分离开来的洞察力与以及(和Hado van Hasselt一起)提供的呈现在[第12章](to chapter12)中关于资格迹的前向视图与后向视图的等价性的观点. Sutton也要感谢Alberta政府与加拿大国家科技研究议会&lt;National Science and Engineering Research Council of Canada&gt;在本版的构思与书写期间提供的支持与自由. Sutton也特别感激Randy Goebel创造的Alberta处富有支持性与远见的学术环境. Sutton也要感谢DeepMind在本书书写的最后6个月提供的帮助.

最后, 我们要感谢许许多多的本版在线草稿的读者. 他们发现了我们遗漏的很多错误, 也就一些潜在的困惑点向我们发出警告.